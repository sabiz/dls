{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification by Convolutional Neural Network\n",
    "\n",
    "http://tkengo.github.io/blog/2016/03/14/text-classification-by-cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from janome.tokenizer import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"./word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs =[\n",
    "    \"最近、筆者は1人用個室の仕事場を使っているのですが、ここにGoogle Homeを設置してみました。一般のオフィスやコワーキングスペースでは難しそうですが、自分しかいない部屋なら気兼ねなく活用できます。\",\n",
    "    \"近年、地球の衛星写真を閲覧できるサービス『Google Earth』から、謎の物体の発見報告が相次いでいる。今回は、なんと南極大陸近くの水中から”謎の巨大UFO”が発見されたというのだ\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "TODO \n",
    "テキストデータを読んで単語ごとのWordVectorの行列にして返す\n",
    "W2Vのモデルを作る\n",
    "\n",
    "文章を単語で分割しそれぞれの単語のWord2Vecの値をくっつけたものを\n",
    "配列にして返却\n",
    "\n",
    "戻り値のイメージ↓\n",
    "[[ 0.09960286  0.14075445 -0.01279956 ...,  0.          0.          0.        ]\n",
    " [ 0.07544287  0.03670182 -0.039662   ...,  0.          0.          0.        ]]\n",
    "  ↑は Word2Vecの次元数 * 100(適当)　のサイズ\n",
    "  サイズが足りなければ(文章が短い) 0でpadding\n",
    "\n",
    "'''\n",
    "ROW_SIZE = 100\n",
    "W2V_SIZE = 50      #Word2Vecの次元数[size]\n",
    "def loadData():\n",
    "    result = []\n",
    "    tokenizer = Tokenizer()\n",
    "    for doc in docs:\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        r = []\n",
    "        index = 0\n",
    "        for token in tokens:\n",
    "            if(token.part_of_speech.split(',')[0] != \"記号\"):\n",
    "                r.extend(model.wv[token.surface])\n",
    "                index+=1\n",
    "                if(ROW_SIZE <= index):break\n",
    "        if(0<index):\n",
    "            for i in range(ROW_SIZE - index):r.extend(np.zeros(W2V_SIZE)) #Padding\n",
    "            result.append(r)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "\n",
    "data = loadData()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5000)\n",
      "[[ 0.09960286  0.14075445 -0.01279956 ...,  0.          0.          0.        ]\n",
      " [ 0.07544287  0.03670182 -0.039662   ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data)\n",
    "\n",
    "# ---------------------------\n",
    "# Input layer\n",
    "# ---------------------------\n",
    "CLASS_NUM = 3\n",
    "dimX= data.shape[0]\n",
    "inX = tf.placeholder(tf.float32, [None, dimX])\n",
    "inY = tf.placeholder(tf.float32, [None, CLASS_NUM]) # 分類したいクラス数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Tensor(\"Placeholder_36:0\", shape=(?, 2), dtype=float32)\n",
      "Tensor(\"Placeholder_37:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"conv-3_16/MaxPool:0\", shape=(?, 2, 5000, 5000), dtype=float32)\n",
      "Tensor(\"conv-4_14/MaxPool:0\", shape=(?, 2, 5000, 5000), dtype=float32)\n",
      "Tensor(\"conv-5_14/MaxPool:0\", shape=(?, 2, 5000, 5000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(dimX)\n",
    "print(inX)\n",
    "print(inY)\n",
    "\n",
    "# ---------------------------\n",
    "# Convolutional & Pooling layer\n",
    "# ---------------------------\n",
    "FILTER_SIZES = [ 3, 4, 5 ]\n",
    "array = []\n",
    "#inXv = tf.convert_to_tensor(data)\n",
    "\n",
    "FILTER_NUM = W2V_SIZE * ROW_SIZE\n",
    "x = tf.placeholder(tf.float32, [ None, dimX, FILTER_NUM, 1])\n",
    "\n",
    "for filterSize in FILTER_SIZES:\n",
    "    with tf.name_scope('conv-%d' % filterSize):\n",
    "        w = tf.Variable(tf.truncated_normal(\n",
    "                                [ filterSize,  FILTER_NUM, 1, FILTER_NUM],\n",
    "                                stddev=0.02), name='weight')\n",
    "        b  = tf.Variable(tf.constant(0.1, shape=[ FILTER_NUM ]), name='bias')\n",
    "        c0 = tf.nn.conv2d(x, w, [ 1, 1, 1, 1 ], 'SAME')\n",
    "        c1 = tf.nn.relu(tf.nn.bias_add(c0, b))\n",
    "        c2 = tf.nn.max_pool(c1, [ 1, dimX - filterSize + 1, 1, 1 ], [ 1, 1, 1, 1 ], 'SAME')\n",
    "        print(c2)\n",
    "        array.append(c2)\n",
    "\n",
    "p = tf.concat(array,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Fully-connected & Output layer\n",
    "# ---------------------------\n",
    "keep = tf.placeholder(tf.float32)\n",
    "with tf.name_scope('fc'):\n",
    "    totalFilters = FILTER_NUM * len(FILTER_SIZES)\n",
    "    w = tf.Variable(tf.truncated_normal([ totalFilters, CLASS_NUM ], stddev=0.02), name='weight')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[ CLASS_NUM ]), name='bias')\n",
    "    h0 = tf.nn.dropout(tf.reshape(p, [ -1, totalFilters ]), keep)\n",
    "    predict_y = tf.nn.softmax(tf.matmul(h0, w) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Optimizer\n",
    "# ---------------------------\n",
    "L2_LAMBDA = 0.0001\n",
    "xentropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict_y,labels=inY))\n",
    "\n",
    "loss = xentropy + L2_LAMBDA * tf.nn.l2_loss(w)\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "train = tf.train.AdamOptimizer(0.0001).minimize(loss, global_step=global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'scalar_summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-8b67e59425f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss_sum\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0maccr_sum\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mt_loss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'general loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'scalar_summary'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Measurement of accuracy and summary for TensorBoard.\n",
    "# ----------------------------------------------------------\n",
    "predict  = tf.equal(tf.argmax(predict_y, 1), tf.argmax(inY, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))\n",
    "\n",
    "loss_sum   = tf.summary.scalar('train loss', loss)\n",
    "accr_sum   = tf.summary.scalar('train accuracy', accuracy)\n",
    "t_loss_sum = tf.summary.scalar('general loss', loss)\n",
    "t_accr_sum = tf.scalar_summary('general accuracy', accuracy)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Start TensorFlow Session.\n",
    "# ----------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter(SUMMARY_LOG_DIR, sess.graph_def)\n",
    "\n",
    "    train_x_length = len(train_x)\n",
    "    batch_count = int(train_x_length / NUM_MINI_BATCH) + 1\n",
    "\n",
    "    log('Start training.')\n",
    "    log('     epoch: %d' % NUM_EPOCHS)\n",
    "    log('mini batch: %d' % NUM_MINI_BATCH)\n",
    "    log('train data: %d' % train_x_length)\n",
    "    log(' test data: %d' % len(test_x))\n",
    "    log('We will loop %d count per an epoch.' % batch_count)\n",
    "\n",
    "    # Start training. We will loop some epochs.\n",
    "    for epoch in xrange(NUM_EPOCHS):\n",
    "        # Randomize training data every epoch in order to converge training more quickly.\n",
    "        random_indice = np.random.permutation(train_x_length)\n",
    "\n",
    "        # Split training data into mini batch for SGD.\n",
    "        log('Start %dth epoch.' % (epoch + 1))\n",
    "        for i in xrange(batch_count):\n",
    "            # Take mini batch from training data.\n",
    "            mini_batch_x = []\n",
    "            mini_batch_y = []\n",
    "            for j in xrange(min(train_x_length - i * NUM_MINI_BATCH, NUM_MINI_BATCH)):\n",
    "                mini_batch_x.append(train_x[random_indice[i * NUM_MINI_BATCH + j]])\n",
    "                mini_batch_y.append(train_y[random_indice[i * NUM_MINI_BATCH + j]])\n",
    "\n",
    "            # TRAINING.\n",
    "            _, v1, v2, v3, v4 = sess.run(\n",
    "                [ train, loss, accuracy, loss_sum, accr_sum ],\n",
    "                feed_dict={ input_x: mini_batch_x, input_y: mini_batch_y, keep: 0.5 }\n",
    "            )\n",
    "            log('%4dth mini batch complete. LOSS: %f, ACCR: %f' % (i + 1, v1, v2))\n",
    "\n",
    "            # Write out loss and accuracy value into summary logs for TensorBoard.\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            writer.add_summary(v3, current_step)\n",
    "            writer.add_summary(v4, current_step)\n",
    "\n",
    "            # Save all variables to a file every checkpoints.\n",
    "            if current_step % CHECKPOINTS_EVERY == 0:\n",
    "                saver.save(sess, CHECKPOINTS_DIR + '/model', global_step=current_step)\n",
    "                log('Checkout was completed.')\n",
    "\n",
    "            # Evaluate the model by test data every evaluation point.\n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                random_test_indice = np.random.permutation(100)\n",
    "                random_test_x = test_x[random_test_indice]\n",
    "                random_test_y = test_y[random_test_indice]\n",
    "\n",
    "                v1, v2, v3, v4 = sess.run(\n",
    "                    [ loss, accuracy, t_loss_sum, t_accr_sum ],\n",
    "                    feed_dict={ input_x: random_test_x, input_y: random_test_y, keep: 1.0 }\n",
    "                )\n",
    "                log('Testing... LOSS: %f, ACCR: %f' % (v1, v2))\n",
    "                writer.add_summary(v3, current_step)\n",
    "                writer.add_summary(v4, current_step)\n",
    "\n",
    "    # Save the model before the program is finished.\n",
    "saver.save(sess, CHECKPOINTS_DIR + '/model-last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
