{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習済みWord2Vec\n",
    " [東北大学 乾・岡崎研究室](http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/)\n",
    " \n",
    " [白ヤギコーポレーション](https://github.com/shiroyagicorp/japanese-word2vec-model-builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのロード\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "#model = KeyedVectors.load_word2vec_format(\"./entity_vector.model.bin\", binary = True)\n",
    "\n",
    "model = Word2Vec.load(\"./word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('問題', 0.7855098247528076),\n",
       " ('問題点', 0.7577674388885498),\n",
       " ('施策', 0.7432342171669006),\n",
       " ('現状', 0.7371587157249451),\n",
       " ('要因', 0.7271366119384766),\n",
       " ('解決策', 0.7200082540512085),\n",
       " ('方策', 0.7162488102912903),\n",
       " ('状況', 0.7085295915603638),\n",
       " ('成果', 0.7010855674743652),\n",
       " ('あり方', 0.6948018670082092)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('課題')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "def toWakatikiCleaned(doc):\n",
    "    # Execute wakati / mecab-ipadic-neologd\n",
    "    tagger = MeCab.Tagger(\"-d /usr/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(doc)\n",
    "    #代名詞はずす？\n",
    "    excludeList = [\"数\", \"接尾\", \"非自立\", \"代名詞\", \"形容動詞語幹\",\"固有名詞\"]\n",
    "    result = []\n",
    "    fullresult = []\n",
    "    node = node.next\n",
    "    while node:\n",
    "        meta = node.feature.split(\",\")\n",
    "        if meta[0] == \"名詞\" and not meta[1] in excludeList :\n",
    "            #print(node.feature)\n",
    "            #print(node.surface)\n",
    "            result.append(node.surface)\n",
    "        fullresult.append(node.surface)\n",
    "        node = node.next\n",
    "    return fullresult,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toWakatiki(doc):\n",
    "    # Execute wakati / mecab-ipadic-neologd\n",
    "    tagger = MeCab.Tagger(\"-d /usr/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    tagger.parse('') # For mecab bug :<\n",
    "    node = tagger.parseToNode(doc)\n",
    "    result = []\n",
    "    node = node.next\n",
    "    while node:\n",
    "        result.append(node.surface)\n",
    "        node = node.next\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcSimilarity(token, target,doc):\n",
    "    result = 0\n",
    "    try:\n",
    "        result = model.similarity(token, target)\n",
    "    except:\n",
    "        pass\n",
    "        #　追加学習で単語の追加はできない？\n",
    "        #model.build_vocab(doc, update=True)\n",
    "        #model.train(doc, total_examples = sum([len(wakati) for wakati in doc]), epochs = model.iter,compute_loss = False)\n",
    "        #print(token)\n",
    "        #return calcSimilarity(token,target,doc)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def calcTfIdf(docs):\n",
    "    tokens = []\n",
    "    for doc in docs:\n",
    "        tokens.append(\" \".join(toWakatiki(doc)))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(tokens)\n",
    "    #print(vectorizer.vocabulary_)\n",
    "    return tfidf,vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def doRecord(docs):\n",
    "    tfidf,vectrorizer = calcTfIdf(docs)\n",
    "    result = []\n",
    "    for doc in docs:\n",
    "        fulltokens,tokens = toWakatikiCleaned(doc)\n",
    "        similarityK = []\n",
    "        similarityT = []\n",
    "        similarityS = []\n",
    "        weight = []\n",
    "        for token in tokens:\n",
    "            similarityK.append(calcSimilarity(token,\"課題\",fulltokens))\n",
    "            similarityT.append(calcSimilarity(token,\"todo\",fulltokens))\n",
    "            similarityS.append(calcSimilarity(token,\"質問\",fulltokens))\n",
    "            try:\n",
    "                weight.append(vectrorizer.vocabulary_[token])\n",
    "            except:\n",
    "                weight.append(0)\n",
    "        nsimilarityK = np.array(similarityK)\n",
    "        nsimilarityT = np.array(similarityT)\n",
    "        nsimilarityS = np.array(similarityS)\n",
    "        nweight = np.array(weight)\n",
    "        tmpK = ((nsimilarityK * nweight).sum() / nweight.sum())\n",
    "        tmpT = ((nsimilarityT * nweight).sum() / nweight.sum())\n",
    "        tmpS = ((nsimilarityS * nweight).sum() / nweight.sum())\n",
    "        dic = {tmpK:\"課題\",tmpT:\"todo\",tmpS:\"質問\"}\n",
    "        result.append(dic[max(tmpK,tmpT,tmpS)])\n",
    "    print(result)\n",
    "    # TODO\n",
    "    # 再学習\n",
    "    # 特徴量を計算　（文章単位/fulltokens）\n",
    "    # 課題,アクションアイテム,QA　それぞれを　単語単位で word2vecで類似度を計算　（単語単位/ tokens）\n",
    "    # 特徴量と加重平均をとる "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['質問', '質問', '課題', '課題', '質問', '課題', '課題']\n"
     ]
    }
   ],
   "source": [
    "doRecord([#\"Todoの中から優先してやらなければならないことを決めよう。\",\n",
    "\"アマゾンジャパンの通販サイトで購入した任天堂の家庭用ゲーム機が届かないまま返送されたとの顧客の苦情が、インターネットの投稿サイトなどに相次ぎ寄せられたことが７日、分かった。配送トラブルの可能性がある。\",\n",
    "\"「ほんやくコンニャク」は、見た目こそコンニャクそのものだが、これを食べると、外国語を自国語として聞き取れるようになり、話し言葉も自動的に話し相手の言語に翻訳されるため、言葉の通じない相手と会話できるようになる。\",\n",
    "\"発表された車種リストには、同じ車種だがレースカテゴリやクラスごとに異なるマシンも含まれている。詳しくは下のビデオをご覧いただきたい。\",\n",
    "\"その目的は、国土交通省に申請・届出した通りに生産されているかどうかを確認すること。\",\n",
    "\"いずれにしても、新iPhoneを買うべきなのかどうか、あるいはもし買うならどちらか、逡巡している人はまだ多いのかもしれない。\",\n",
    "\"デザインも存在感が少なく、日本の家庭にはこちらの方がはるかに適していると言えるのではないか。\",\n",
    "\"AIが最重要技術であるとはいえ、AIの活用に関しては問題が山積した状態です。例えば、フェイクニュースの問題は、検索サービスやニュース記事サービスを提供するGoogleにとって解決すべき喫緊の課題で、AIを活用することで誤った情報を排除して質の高いものだけを選別する試みが行われています。\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
