{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参考](https://qiita.com/Hironsan/items/2466fe0f344115aff177)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データロード (nucc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def loadNucc():\n",
    "    data = []\n",
    "    for file in glob.glob('./nucc/*'):\n",
    "        d = open(file)\n",
    "        data.append(d.read())\n",
    "        d.close()\n",
    "        # break\n",
    "    return data\n",
    "docData = loadNucc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クリーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanDocNucc(docData):\n",
    "    for idx,doc in enumerate(docData):\n",
    "        # ヘッダー & 話者プレフィックス削除\n",
    "        tmp = re.sub(re.compile(r'^＠.*|^[FM](\\d{3})：|^Ｘ：', re.MULTILINE),\"\",doc)\n",
    "        # （おー）/＜笑い＞など削除\n",
    "        tmp = re.sub(re.compile(r'（[^（]+）|＜[^＜]+＞', re.MULTILINE),\"\",tmp)\n",
    "        # ＊置換\n",
    "        tmp = re.sub(re.compile(r'＊', re.MULTILINE),\"\",tmp)\n",
    "        # 改行を削除\n",
    "        tmp = re.sub(re.compile(r'\\n', re.MULTILINE),\" \",tmp)\n",
    "        docData[idx] = tmp\n",
    "    return docData\n",
    "\n",
    "docData = cleanDocNucc(docData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 単語に分割(janome編)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "def toWakati(docData):\n",
    "    result = []\n",
    "    tokenizer = Tokenizer()\n",
    "    for doc in docData:\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        r = []\n",
    "        for token in tokens:\n",
    "            if(token.part_of_speech.split(',')[0] != \"記号\"):\n",
    "                r.append(token.surface)\n",
    "        if(0<len(r)):\n",
    "            result.append(r)\n",
    "    return result\n",
    "docData = toWakati(docData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語の正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(docData):\n",
    "    #文字種の統一 ？\n",
    "    #数字の置き換え　？\n",
    "    #単語の統一　（ソニー > sony　みたいな）\n",
    "    return docData\n",
    "\n",
    "docData = normalize(docData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "def word2vec(docData):\n",
    "    sentences = []\n",
    "    for idx,doc in enumerate(docData):\n",
    "        sentences.append(LabeledSentence(words=doc, tags=[idx]))\n",
    "    model = models.Doc2Vec(sentences, dm=0, size=300, window=15, alpha=.025,\n",
    "        min_alpha=.025, min_count=1, sample=1e-6)\n",
    "    for epoch in range(200):\n",
    "        print('Epoch: {}'.format(epoch + 1))\n",
    "        model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.alpha -= (0.025 - 0.0001) / 19\n",
    "        model.min_alpha = model.alpha\n",
    "        \n",
    "    model.save(\"word2vec.model\")\n",
    "word2vec(docData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
